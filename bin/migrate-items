#!/usr/bin/env ruby
# frozen_string_literal: true

# Usage:
#   RAILS_ENV=production bin/migrate-items items.jsonl 0 /opt/app/local-files migration_item_type_updates.csv 2>&1 | tee -a migrate-items.log
#
#   These two just load metadata:
#
#   RAILS_ENV=production bin/migrate-items items.jsonl 1000
#   RAILS_ENV=production bin/migrate-items items.jsonl

require 'csv'
require_relative '../config/environment'

Rails.logger.level = :warn

DEFAULT_DATA_ROOT = '/data/hydrus-files'
DATA_ROOT = ARGV[2]
CSV_FILE = ARGV[3]

warn "A data files was not provided" unless CSV_FILE
warn "A data root was not provided, so no files will be imported" unless DATA_ROOT

STATES = {
  'published' => 'deposited',
  'draft' => 'first_draft',
  'awaiting_approval' => 'pending_approval',
  'returned' => 'rejected'
}

LICENSE = {
  'pddl' => 'PDDL-1.0',
  'cc-by' => 'CC-BY-3.0',
  'cc-by-sa' => 'CC-BY-SA-3.0',
  'cc-by-nc' => 'CC-BY-NC-3.0',
  'cc-by-nc-sa' => 'CC-BY-NC-SA-3.0',
  'cc-by-nc-nd' => 'CC-BY-NC-ND-3.0',
  'cc-by-nd' => 'CC-BY-ND-3.0',
  'cc-pdm' => 'PDDL-1.0',
  'cc-cc0' => 'CC0-1.0',
  'odc-by' => 'ODC-By-1.0',
  'odc-odbl' => 'ODbL-1.0',
  'none' => 'none'
}

WORK_TYPE = {
  'dataset' => 'data',
  'image' => 'image',
  'video' => 'video',
  'audio - spoken' => 'sound',
  'audio - music' => 'sound',
  'other' => 'other',
  'class project' => 'other',
  'textbook' => 'text',
  'thesis' => 'text',
  'article' => 'text',
  'technical report' => 'text',
  'conference paper / presentation' => 'other',
  'software' => 'software, multimedia',
  'computer game' => 'software, multimedia',
  'archival mixed material' => 'other'
}

CONTRIBUTOR_TYPE = {
  'personal' => 'person',
  'corporate' => 'organization',
  'conference' => 'organization'
}

CONTRIBUTOR_ROLE = {
  'Advisor' => 'Thesis advisor',
  'Author' => 'Author',
  'Conference' => 'Conference',
  'Contributing author' => 'Contributing author',
  'contributor' => 'Contributing author',
  'Contributor' => 'Contributing author',
  'Creator' => 'Creator',
  'Degree granting institution' => 'Degree granting institution',
  'Department' => 'Department',
  'Distributor' => 'Distributor',
  'Editor' => 'Editor',
  'Primary advisor' => 'Primary thesis advisor',
  'Principal investigator' => 'Principal investigator',
  'Publisher' => 'Publisher',
  'Sponsor' => 'Sponsor',
  'aut' => 'Author', # Bad data? about 1000 items are like this.
  'author' => 'Author', # Bad data? about 1000 items are like this.
  'author,' => 'Author', # Bad data? 4 items have this role
  'author.' => 'Author', # Bad data? 207 items have this role
  'autor' => 'Author', # Bad data? 7 items have this role
  'Collector' => 'Data collector',
  'composer' => 'Composer',
  'composer.' => 'Composer',
  'conference' => 'Conference',
  'creator' => 'Creator', # Bad data? 3 items have this role
  'editor' => 'Editor',
  'editor.' => 'Editor',
  'issuing body' => "Issuing body",
  'issuing body,' => "Issuing body",
  'issuing body.' => "Issuing body",
  'issuing boyd.' => "Issuing body", # bad data: druid:nn568sd8519
  'photographer,' => 'Photographer', # bad data? druid:hs463hp0639, druid:wz110cw1514
  'principal investigator and author.' => 'Principal investigator',
  'publisher' => 'Publisher',
  'publisher,' => 'Publisher',
  'publisher.' => 'Publisher',
  'researcher' => 'Researcher',
  'researcher.' => 'Researcher',
  'speaker.' => 'Speaker', # Bad data? druid:kp775fq5022
  'sponsor.' => 'Sponsor',
  'sponsoring and issuing body.' => 'Sponsor',
  'sponsoring body' => 'Sponsor',
  'sponsoring body.' => 'Sponsor',
}

AUTHOR_ROLES = [
  'Author',
  'Composer',
  'Conference',
  'Contributing author',
  'Creator',
  'Degree granting institution',
  'Editor',
  'Issuing body',
  'Photographer',
  'Principal investigator',
  'Publisher',
  'Researcher',
  'Speaker',
  'Sponsor'
]

INVALID_ROLES = [
  'production company.',
  'compiler',
  'compiler.',
  'designer.',
  'printer.',
  'binder.',
  'participant in a treaty.',
  'enacting jurisdiction.',
  'writer of forward.',
  'addressee',
  'addressee.',
  'commissioning body.',
  'commissioning body,',
  'commissioning body',
  'cre', # see druid:jn023kf3320
  'his', # see druid:jn023kf3320
  'ive', # see druid:fg188hk6403 and druid:hd575hm6964
  'ivr', # see druid:fg188hk6403 and druid:hd575hm6964
  'pbl', # see druid:fg188hk6403 and druid:hd575hm6964
  'spk', # on 15 items
  'spn', # on 15 items
  'co-investigator.',
  'Programmer',
  'judge',
  'defendant',
  'plaintiff',
  'writer of added commentary'
]

def load_type_data
  {}.tap do |data|
    CSV.foreach(CSV_FILE, headers: true) do |row|
      druid = "druid:#{row['item_druid']}"
      data[druid] = {
        work_type: row['New Work Type'].downcase,
        subtype: [row['New Subtype 1'], row['New Subtype 2'], row['New Subtype 3'], row['New Subtype 4']].compact
      }
    end
  end
end

def contact_email(json)
  return ContactEmail.new(email: json['contact_email']) if URI::MailTo::EMAIL_REGEXP.match(json['contact_email'])

  warn "Invalid contact email found in #{json['druid']}: #{json['contact_email']}. Setting a dummy value."

  ContactEmail.new(email: 'no-reply@sdr.stanford.edu')
end

def abstract(json)
  return json['abstract'] if json['abstract']

  warn "Work has no abstract: #{json['druid']}. Setting a dummy value"
  'No abstract entered'
end

def add_events!(work, json)
  json.fetch('events').each do |event|
    work.events.build(
      user: User.find_or_create_by!(email: "#{event['who'].downcase}@stanford.edu"),
      created_at: Time.parse(event['when']),
      updated_at: Time.parse(event['when']),
      event_type: 'update_metadata', # is this OK?
      description: event['text']
    )
  end
end

def license(json)
  LICENSE.fetch(json.fetch('license'))
end

def split_name(full_name)
  if full_name.include?(',')
    last, first = full_name.split(',').map(&:strip)
    { last_name: last, first_name: first }
  else
    first, last = full_name.split(' ')
    { last_name: last, first_name: first }
  end
end

def contributors_and_authors(json)
  data = {
    contributors: [],
    authors: []
  }

  json.fetch('contributors').map do |attributes|
    contributor_attributes = attributes.dup
    hydrus_role = contributor_attributes.delete('role')
    case hydrus_role
    when *INVALID_ROLES # H2 doesn't have an equivalent role
      warn "Found role '#{hydrus_role}' for #{json['druid']}, but there is no H2 equivalent role. Skipping."
      next
    when ''
      warn "Found empty role for #{json['druid']}, Skipping."
      next
    end
    role = CONTRIBUTOR_ROLE.fetch(hydrus_role)

    type = CONTRIBUTOR_TYPE.fetch(contributor_attributes.delete('name_type'))
    if type == 'person'
      contributor_attributes.merge!(split_name(contributor_attributes.delete('full_name')))
    end
    if role.in?(AUTHOR_ROLES)
      data[:authors] << Author.new(contributor_attributes.merge(contributor_type: type, role: role))
    else
      data[:contributors] << Contributor.new(contributor_attributes.merge(contributor_type: type, role: role))
    end
  end

  data
end

def related_links(json)
  json.fetch('related_items').reject { |item| item['url'].blank? }.map { |item| RelatedLink.new(item) }
end

def related_works(json)
  json.fetch('related_citations').reject(&:blank?).map { |citation| RelatedWork.new(citation: citation) }
end

def build_work_attributes(json)
  collection_id = json['collection']
  unless collection_id
    warn "No collection exists for #{json['druid']}. Skipping."
    return
  end

  collection = Collection.find_by(druid: collection_id)
  unless collection
    warn "Unable to find Collection #{collection_id}. Skipping"
    return
  end

  depositor = User.find_or_create_by!(email: "#{json.dig('creator', 'sunetid')}@stanford.edu")
  {
    depositor: depositor,
    collection: collection,
    created_at: Time.parse(json['created_at']),
    updated_at: Time.parse(json['updated_at'])
  }
end

def build_work_version_attributes(json, type_data)
  work_type = WORK_TYPE.fetch(json.fetch('item_type'))
  subtype = 'nil'

  # Overwrite the values if there was a row in the CSV for this item
  if row = type_data[json['druid']]
    work_type = row.fetch(:work_type)
    subtype = row.fetch(:subtype)
  else
    subtype = if work_type == 'other'
      warn "#{json['druid']} has a work_type of 'other', but Hydrus doesn't specify a subtype.  Setting a placeholder."
      ['other']
    else
      case json.fetch('item_type')
      when 'thesis', 'article', 'technical report'
        [json.fetch('item_type').capitalize]
      when 'textbook'
        ['Book']
      end
    end
  end

  title = json['title']
  unless title
    warn "#{json['druid']} does not have a title. #{json}"
    return
  end

  created_edtf = EDTF.parse(json.fetch('date_created')) if json['date_created']
  {
    version: json.fetch('version'),
    abstract: abstract(json),
    contact_emails: [contact_email(json)],
    license: license(json),
    access: json.fetch('visibility', 'stanford'),
    title: title,
    work_type: work_type,
    subtype: subtype,
    citation: json['citation'],
    keywords: json.fetch('keywords').map { |text| Keyword.new(label: text) },
    created_edtf: created_edtf,
    state: STATES.fetch(json.fetch('object_status')),
    related_links: related_links(json),
    related_works: related_works(json),
    created_at: Time.parse(json['created_at']),
    updated_at: Time.parse(json['updated_at'])
  }.merge(contributors_and_authors(json))
end

def resolve_file(hydrus_path)
  hydrus_path.sub(DEFAULT_DATA_ROOT, DATA_ROOT)
end

def attach_files(work_version, files)
  files.each do |file_data|
    attached_file = work_version.attached_files.find { |attached| attached.filename == file_data.fetch(:filename) }
    next if attached_file

    attached_file = work_version.attached_files.build(label: file_data.fetch(:label), hide: file_data.fetch(:hide))
    attached_file.file.attach(io: file_data.fetch(:file), filename: file_data.fetch(:filename))
    attached_file.save!
  end
end

def build_files(files)
  return [] unless DATA_ROOT

  files.map do |file_data|
    pathname = resolve_file(file_data['path'])
    filename = File.basename(pathname)
    {
      file: File.open(pathname),
      filename: filename,
      label: file_data['label'],
      hide: file_data['hide']
    }
  end
end

# This is similar to File.open do ... end but with a bunch of files
def with_files(json_files)
  files = build_files(json_files)
  yield files
  files.each { |data| data.fetch(:file).close }
end

def create_or_update_work(json, type_data)
  work = Work.find_by(druid: json['druid'])
  # Files must be open until the transaction is closed
  with_files(json['files']) do |files|
    ActiveRecord::Base.transaction do
      if work
        unless work.update(build_work_attributes(json))
          warn "Validation failed on #{json['druid']}, #{work.errors.full_messages}"
          return
        end
      else
        work = Work.new(build_work_attributes(json).merge(druid: json['druid']))
        unless work.save
          warn "Validation failed on #{json['druid']}, #{work.errors.full_messages}"
          return
        end
      end

      work_version = work.head
      if work_version
        work_version.update!(build_work_version_attributes(json, type_data))
        attach_files(work_version, files)
      else
        work_version = work.build_head(build_work_version_attributes(json, type_data).
          merge(work_id: work.id, version: json.fetch('version')))
        work_version.save!
        attach_files(work_version, files)
        work.update!(head: work_version)
      end
      puts "Work #{work.id} (v#{work_version.version}): #{work.druid}"
    end
  end
end

puts "STARTING NEW WORK MIGRATION: #{Time.now}"
type_data = load_type_data
OFFSET = ARGV[1].present? ? ARGV[1].to_i : 0
count = 0
File.foreach(ARGV[0]) do |data|
  count += 1
  next if count < OFFSET
  puts count
  json = JSON.parse(data)
  next unless json.present?
  create_or_update_work(json, type_data)
end
